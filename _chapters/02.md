---
title: Distributed Representation
---

## B. Word2vec

As shown in , the CBOW model is a simple fully connected neural network with one hidden layer. The input layer, which takes the one-hot vector of context word has $${\bf V}$$ neurons while the hidden layer has $${\bf N}$$ neurons. The output layer is softmax of all words in the vocabulary. The layers are connected by weight matrix 
$${\bf W} \in \mathcal{R}^{V \times N }$$ and $${\bf W^{'}} \in \mathcal{R}^{H \times V}$$, respectively. Each word from the vocabulary is finally represented as two learned vectors $${\bf v_c}$$ and $${\bf v_w}$$, corresponding to context and target word representations, respectively. Thus, $$k^{th}$$ word in the vocabulary will have
