---
title: Distributed Representation
---

Statistical NLP has emerged as the primary option for modeling complex natural language tasks. However, in its beginning, it often used to suffer from the notorious *curse of dimensionality* while learning joint probability functions of language models. This led to the motivation of learning distributed representations of words existing in low-dimensional space [CITE]. 

## Word Embeddings

Distributional vectors or word embeddings (Fig. [REF]) essentially follow the distributional hypothesis, according to which words with similar meanings tend to occur in similar context. Thus, these vectors try to capture the characteristics of the neighbors of a word.
The main advantage of distributional vectors is that they capture similarity between words. Measuring similarity between vectors is possible, using measures such as cosine similarity.
Word embeddings are often used as the first data processing layer in a deep learning model. Typically, word embeddings are pre-trained by optimizing an auxiliary objective in a large unlabeled corpus, such as predicting a word based on its context [CITE], where the learned word vectors can capture general syntactical and semantic information. Thus, these embeddings have proven to be efficient in capturing context similarity, analogies and due to its smaller dimensionality, are fast and efficient in computing core NLP tasks.

Over the years, the models that create such embeddings have been shallow neural networks and there has not been need for deep networks to create good embeddings. However, deep learning based NLP models invariably represent their words, phrases and even sentences using these embeddings. This is in fact a major difference between traditional word count based models and deep learning based models. Word embeddings have been responsible for state-of-the-art results in a wide range of NLP tasks [CITE]. 

![alt txt](img/distributional.png)
*Figure 2: Distributional vectors represented by a $${\bf D}$$-dimensional vector where $${\bf D} << {\bf V}$$, where $${\bf V}$$ is size of Vocabulary. Figure Source: [http://veredshwartz.blogspot.sg](http://veredshwartz.blogspot.sg).*

### B. Word2vec

As shown in , the CBOW model is a simple fully connected neural network with one hidden layer. The input layer, which takes the one-hot vector of context word has $${\bf V}$$ neurons while the hidden layer has $${\bf N}$$ neurons. The output layer is softmax of all words in the vocabulary. The layers are connected by weight matrix 
$${\bf W} \in \mathcal{R}^{V \times N }$$ and $${\bf W^{'}} \in \mathcal{R}^{H \times V}$$, respectively. Each word from the vocabulary is finally represented as two learned vectors $${\bf v_c}$$ and $${\bf v_w}$$, corresponding to context and target word representations, respectively. Thus, $$k^{th}$$ word in the vocabulary will have
