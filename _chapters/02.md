---
title: Distributed Representation
---

Statistical NLP has emerged as the primary option for modeling complex natural language tasks. However, in its beginning, it often used to suffer from the notorious *curse of dimensionality* while learning joint probability functions of language models. This led to the motivation of learning distributed representations of words existing in low-dimensional space [CITE]. 

## A Word Embeddings

Distributional vectors or word embeddings (Fig. [REF]) essentially follow the distributional hypothesis, according to which words with similar meanings tend to occur in similar context. Thus, these vectors try to capture the characteristics of the neighbors of a word.
The main advantage of distributional vectors is that they capture similarity between words. Measuring similarity between vectors is possible, using measures such as cosine similarity.
Word embeddings are often used as the first data processing layer in a deep learning model. Typically, word embeddings are pre-trained by optimizing an auxiliary objective in a large unlabeled corpus, such as predicting a word based on its context [CITE], where the learned word vectors can capture general syntactical and semantic information. Thus, these embeddings have proven to be efficient in capturing context similarity, analogies and due to its smaller dimensionality, are fast and efficient in computing core NLP tasks.

![alt txt](img/distributional.png)
*Figure 2: Distributional vectors represented by a $${\bf D}$$-dimensional vector where $${\bf D} << {\bf V}$$, where $${\bf V}$$ is size of Vocabulary. Figure Source: [http://veredshwartz.blogspot.sg](http://veredshwartz.blogspot.sg).*

Over the years, the models that create such embeddings have been shallow neural networks and there has not been need for deep networks to create good embeddings. However, deep learning based NLP models invariably represent their words, phrases and even sentences using these embeddings. This is in fact a major difference between traditional word count based models and deep learning based models. Word embeddings have been responsible for state-of-the-art results in a wide range of NLP tasks [CITE]. 

For example, [CITE] used embeddings along with stacked denoising autoencoders for domain adaptation in sentiment classification and [CITE] presented combinatory categorical autoencoders to learn the compositionality of sentence. Their wide usage across the recent literature shows their effectiveness and importance in any deep learning model performing a NLP task. 

Distributed representations (embeddings) are mainly learned through context. During 1990s, several research developments [CITE] marked the foundations of research in distributional semantics. A more detailed summary of these early trends is provided in [CITE]. Later developments were adaptations of these early works, which led to creation of topic models like latent Dirichlet allocation [CITE] and language models [CITE]. These works laid out the foundations of representation learning.

In 2003, [CITE] proposed a neural language model which learned distributed representations for words (Fig. [REF]). Authors argued that these word representations, once compiled into sentence representations using joint probability of word sequences, achieved an exponential number of semantically neighboring sentences. This, in turn, helped in generalization since unseen sentences could now gather higher confidence if word sequences with similar words (in respect to nearby word representation) were already seen. 

![alt txt](img/bengio_Page_1.png)
*Figure 3: Neural Language Model (Figure reproduced from [CITE]). $$C(i)$$ is the $$i^{th}$$ word embedding.*

[CITE] was the first work to show the utility of pre-trained word embeddings. The authors proposed a neural network architecture that forms the foundation to many current approaches. The work also establishes word embeddings as a useful tool for NLP tasks. However, the immense popularization of word embeddings was arguably due to [CITE], who proposed the continuous bag-of-words (CBOW) and skip-gram models to efficiently construct high-quality distributed vector representations. Propelling their popularity was the unexpected side effect of the vectors exhibiting compositionality, i.e., adding two word vectors results in a vector that is a semantic composite of the individual words, e.g., 'man' + 'royal' = 'king'. The theoretical justification for this behavior was recently given by [CITE], which stated that compositionality is seen only when certain assumptions are held, e.g., the assumption that words need to be uniformly distributed in the embedding space. 

[CITE] is another famous word embedding method which is essentially a "count-based" model. Here, the word co-occurrence count matrix is preprocessed by normalizing the counts and log-smoothing them. This matrix is then factorized to get lower dimensional representations which is done by minimizing a "reconstruction loss".

Below, we provide a brief description of the word2vec method proposed by [CITE]. 

## B. Word2vec

Word embeddings were revolutionized by [CITE] who proposed the CBOW and skip-gram models. CBOW computes the conditional probability of a target word given the context words surrounding it across a window of size $$k$$. On the other hand, the skip-gram model does the exact opposite of the CBOW model, by predicting the surrounding context words given the central target word. The context words are assumed to be located symmetrically to the target words within a distance equal to the window size in both directions. In unsupervised settings, the word embedding dimension is determined by the accuracy of prediction. As the embedding dimension increases, the accuracy of prediction also increases until it converges at some point, which is considered the optimal embedding dimension as it is the shortest without compromising accuracy.

Let us consider a simplified version of the CBOW model where only one word is considered in the context. This essentially replicates a bigram language model. 

![alt txt](img/CBOW.png)
*Figure 3: Model for CBOW (Figure source: [REF])*

As shown in Fig. [REF] , the CBOW model is a simple fully connected neural network with one hidden layer. The input layer, which takes the one-hot vector of context word has $${\bf V}$$ neurons while the hidden layer has $${\bf N}$$ neurons. The output layer is softmax of all words in the vocabulary. The layers are connected by weight matrix 
$${\bf W} \in \mathcal{R}^{V \times N }$$ and $${\bf W^{'}} \in \mathcal{R}^{H \times V}$$, respectively. Each word from the vocabulary is finally represented as two learned vectors $${\bf v_c}$$ and $${\bf v_w}$$, corresponding to context and target word representations, respectively. Thus, $$k^{th}$$ word in the vocabulary will have

$$
\begin{gathered}
{\bf v_c} = {\bf W_{(k,.)}} \quad and \quad {\bf v_w} = {\bf W^{'}_{(.,k)}}
\end{gathered}
$$

Overall, for any word $$w_i$$ with given context word $$c$$ as input,

$$
\begin{gathered}
 {\bf P(\frac{w_i}{c})} = {\bf y_i} = \frac{e^{u_i}}{\sum_{i=1}^{V} e^{u_i}} \quad where, {\bf u_i} = {\bf v_{w_i}^T.v_c}
\end{gathered}
$$

The parameters $ {\bf \theta} = {\bf \{V_w, V_c\}}$ are learned by defining the objective function as the log-likelihood and finding its gradient as

$$
\begin{gathered}
{\bf l(\theta)} = {\bf \sum_{w \in Vocabulary} log(P(\frac{w}{c}))} \\
{\bf \frac{\partial l(\theta)}{\partial V_w}} = {\bf V_c(1-P(\frac{w}{c}))}
\end{gathered}
$$

In the general CBOW model, all the one-hot vectors of context words are taken as input simultaneously, i.e,

$$
{\bf h} = {\bf W^T(x_1 + x_2 + ... + x_c)}
$$

One limitation of individual word embeddings is their inability to represent phrases [CITE], where the combination of two or more words (e.g., idioms like "hot potato" or named entities such as "Boston Globe") does not represent the combination of meanings of individual words. One solution to this problem, as explored by [CITE], is to identify such phrases based on word co-occurrence and train embeddings for them separately. More recent methods have explored directly learning n-gram embeddings from unlabeled data [CITE]. 

Another limitation comes from learning embeddings based only on a small window of surrounding words, sometimes words such as *good* and *bad* share almost the same embedding [CITE], which is problematic if used in tasks such as sentiment analysis [CITE]. At times these embeddings cluster semantically-similar words which have opposing sentiment polarities. This leads the downstream model used for the sentiment analysis task to be unable to identify this contrasting polarities leading to poor performance. [CITE] addresses this problem by proposing sentiment specific word embedding (SSWE). Authors incorporate the supervised sentiment polarity of text in their loss functions while learning the embeddings.

A general caveat for word embeddings is that they are highly dependent on the applications in which it is used. [CITE] proposed task specific embeddings which retrain the word embeddings to align them in the current task space. This is very important as training embeddings from scratch requires large amount of time and resource. [CITE] tried to address this issue by proposing *negative sampling* which is nothing but frequency-based sampling of negative terms while training the word2vec model. 

Traditional word embedding algorithms assign a distinct vector to each word. This makes them unable to account for polysemy. In a recent work, [CITE] provided an innovative way to address this deficit. The authors leveraged multilingual parallel data to learn multi-sense word embeddings. For example, the English word bank, when translated to French provides two different words: *banc* and *banque* representing financial and geographical meanings, respectively. Such multilingual distributional information helped them in accounting for polysemy.

Table [REF] provides a directory of existing frameworks that are frequently used for creating embeddings which are further incorporated into deep learning models.

<table>
  <tr>
    <th>Framework</th>
    <th>Language</th>
    <th>Url</th>
  </tr>
  <tr>
    <td>S-Space</td>
    <td>Java</td>
    <td><a href="https://github.com/fozziethebeat/S-Space">https://github.com/fozziethebeat/S-Space</a></td>
  </tr>
  <tr>
    <td>Semanticvectors</td>
    <td>Java</td>
    <td><a href="https://github.com/fozziethebeat/S-Space">https://github.com/semanticvectors/</a></td>
  </tr>
  <tr>
    <td>Gensim</td>
    <td>Python</td>
    <td><a href="https://github.com/fozziethebeat/S-Space">https://radimrehurek.com/gensim/</a></td>
  </tr>
  <tr>
    <td>Pydsm</td>
    <td>Python</td>
    <td><a href="https://github.com/fozziethebeat/S-Space">https://github.com/jimmycallin/pydsm</a></td>
  </tr>
  <tr>
    <td>Dissect</td>
    <td>Python</td>
    <td><a href="https://github.com/fozziethebeat/S-Space">http://clic.cimec.unitn.it/composes/toolkit/</a></td>
  </tr>
  <tr>
    <td>FastText</td>
    <td>Python</td>
    <td><a href="https://github.com/fozziethebeat/S-Space">https://fasttext.cc/</a></td>
  </tr>
</table>

*Table 1: Frameworks providing embedding tools and methods.*

## C. Character Embeddings

Word embeddings are able to capture syntactic and semantic information, yet for tasks such as POS-tagging and NER, intra-word morphological and shape information can also be very useful. Generally speaking, building natural language understanding systems at the character level has attracted certain research attention [CITE]. Better results on morphologically rich languages are reported in certain NLP tasks. [CITE] applied character-level representations, along with word embeddings for NER, achieving state-of-the-art results in Portuguese and Spanish corpora. [CITE] showed positive results on building a neural language model using only character embeddings. [CITE] exploited several embeddings, including character trigrams, to incorporate prototypical and hierarchical information for learning pre-trained label embeddings in the context of NER.

A common phenomenon for languages with large vocabularies is the unknown word issue or out-of-vocabulary word (OOV) issue. Character embeddings naturally deal with it since each word is considered as no more than a composition of individual letters. In languages where text is not composed of separated words but individual characters and the semantic meaning of words map to its compositional characters (such as Chinese), building systems at the character level is a natural choice to avoid word segmentation [CITE]. Thus, works employing deep learning applications on such languages tend to prefer character embeddings over word vectors [CITE]. For example, [CITE] proved that radical-level processing could greatly improve sentiment classification performance. In particular, the authors proposed two types of Chinese radical-based hierarchical embeddings, which incorporate not only semantics at radical and character level, but also sentiment information.[CITE] also tried to improve the representation of words by using character-level information in morphologically-rich languages. They approached the skip-gram method by representing words as bag-of-characters n-grams. Their work thus had the effectiveness of the skip-gram model along with addressing some persistent issues of word embeddings. The method was also fast, which allowed training models on large corpora quickly. Popularly known as *FastText*, such a method stands out over previous methods in terms of speed, scalability, and effectiveness. 

Apart from character embeddings, different approaches have been proposed for OOV handling. [CITE] provided OOV handling on-the-fly by initializing the unknown words as the sum of the context words and refining these words with a high learning rate. However, their approach is yet to be tested on typical NLP tasks. [CITE] provided an interesting approach of training a character-based model to recreate pre-trained embeddings. This allowed them to learn a compositional mapping form character to word embedding, thus tackling the OOV problem.

Despite the ever growing popularity of distributional vectors, recent discussions on their relevance in the long run have cropped up. For example, [CITE] has recently tried to evaluate how well the word vectors capture the necessary facets of conceptual meaning. The authors have discovered severe limitations in perceptual understanding of the concepts behind the words, which cannot be inferred from distributional semantics alone. A possible direction for mitigating these deficiencies will be grounded learning, which has been gaining popularity in this research domain.




