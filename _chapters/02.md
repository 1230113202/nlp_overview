---
title: Distributed Representation
---

Statistical NLP has emerged as the primary option for modeling complex natural language tasks. However, in its beginning, it often used to suffer from the notorious *curse of dimensionality* while learning joint probability functions of language models. This led to the motivation of learning distributed representations of words existing in low-dimensional space [CITE]. 

## A Word Embeddings

Distributional vectors or word embeddings (Fig. [REF]) essentially follow the distributional hypothesis, according to which words with similar meanings tend to occur in similar context. Thus, these vectors try to capture the characteristics of the neighbors of a word.
The main advantage of distributional vectors is that they capture similarity between words. Measuring similarity between vectors is possible, using measures such as cosine similarity.
Word embeddings are often used as the first data processing layer in a deep learning model. Typically, word embeddings are pre-trained by optimizing an auxiliary objective in a large unlabeled corpus, such as predicting a word based on its context [CITE], where the learned word vectors can capture general syntactical and semantic information. Thus, these embeddings have proven to be efficient in capturing context similarity, analogies and due to its smaller dimensionality, are fast and efficient in computing core NLP tasks.

![alt txt](img/distributional.png)
*Figure 2: Distributional vectors represented by a $${\bf D}$$-dimensional vector where $${\bf D} << {\bf V}$$, where $${\bf V}$$ is size of Vocabulary. Figure Source: [http://veredshwartz.blogspot.sg](http://veredshwartz.blogspot.sg).*

Over the years, the models that create such embeddings have been shallow neural networks and there has not been need for deep networks to create good embeddings. However, deep learning based NLP models invariably represent their words, phrases and even sentences using these embeddings. This is in fact a major difference between traditional word count based models and deep learning based models. Word embeddings have been responsible for state-of-the-art results in a wide range of NLP tasks [CITE]. 

For example, [CITE] used embeddings along with stacked denoising autoencoders for domain adaptation in sentiment classification and [CITE] presented combinatory categorical autoencoders to learn the compositionality of sentence. Their wide usage across the recent literature shows their effectiveness and importance in any deep learning model performing a NLP task. 

Distributed representations (embeddings) are mainly learned through context. During 1990s, several research developments [CITE] marked the foundations of research in distributional semantics. A more detailed summary of these early trends is provided in [CITE]. Later developments were adaptations of these early works, which led to creation of topic models like latent Dirichlet allocation [CITE] and language models [CITE]. These works laid out the foundations of representation learning.

In 2003, [CITE] proposed a neural language model which learned distributed representations for words (Fig. [REF]). Authors argued that these word representations, once compiled into sentence representations using joint probability of word sequences, achieved an exponential number of semantically neighboring sentences. This, in turn, helped in generalization since unseen sentences could now gather higher confidence if word sequences with similar words (in respect to nearby word representation) were already seen. 

![alt txt](img/bengio_Page_1.png)
*Figure 3: Neural Language Model (Figure reproduced from [CITE]). $$C(i)$$ is the $$i^{th}$$ word embedding.*

[CITE] was the first work to show the utility of pre-trained word embeddings. The authors proposed a neural network architecture that forms the foundation to many current approaches. The work also establishes word embeddings as a useful tool for NLP tasks. However, the immense popularization of word embeddings was arguably due to [CITE], who proposed the continuous bag-of-words (CBOW) and skip-gram models to efficiently construct high-quality distributed vector representations. Propelling their popularity was the unexpected side effect of the vectors exhibiting compositionality, i.e., adding two word vectors results in a vector that is a semantic composite of the individual words, e.g., 'man' + 'royal' = 'king'. The theoretical justification for this behavior was recently given by [CITE], which stated that compositionality is seen only when certain assumptions are held, e.g., the assumption that words need to be uniformly distributed in the embedding space. 

[CITE] is another famous word embedding method which is essentially a "count-based" model. Here, the word co-occurrence count matrix is preprocessed by normalizing the counts and log-smoothing them. This matrix is then factorized to get lower dimensional representations which is done by minimizing a "reconstruction loss".

Below, we provide a brief description of the word2vec method proposed by [CITE]. 

## B. Word2vec

Word embeddings were revolutionized by [CITE] who proposed the CBOW and skip-gram models. CBOW computes the conditional probability of a target word given the context words surrounding it across a window of size $$k$$. On the other hand, the skip-gram model does the exact opposite of the CBOW model, by predicting the surrounding context words given the central target word. The context words are assumed to be located symmetrically to the target words within a distance equal to the window size in both directions. In unsupervised settings, the word embedding dimension is determined by the accuracy of prediction. As the embedding dimension increases, the accuracy of prediction also increases until it converges at some point, which is considered the optimal embedding dimension as it is the shortest without compromising accuracy.

Let us consider a simplified version of the CBOW model where only one word is considered in the context. This essentially replicates a bigram language model. 

![alt txt](img/CBOW.png)
*Figure 3: Model for CBOW (Figure source: [REF])*

As shown in Fig. [REF] , the CBOW model is a simple fully connected neural network with one hidden layer. The input layer, which takes the one-hot vector of context word has $${\bf V}$$ neurons while the hidden layer has $${\bf N}$$ neurons. The output layer is softmax of all words in the vocabulary. The layers are connected by weight matrix 
$${\bf W} \in \mathcal{R}^{V \times N }$$ and $${\bf W^{'}} \in \mathcal{R}^{H \times V}$$, respectively. Each word from the vocabulary is finally represented as two learned vectors $${\bf v_c}$$ and $${\bf v_w}$$, corresponding to context and target word representations, respectively. Thus, $$k^{th}$$ word in the vocabulary will have

$$
\begin{gathered}
{\bf v_c} = {\bf W_{(k,.)}} \quad and \quad {\bf v_w} = {\bf W^{'}_{(.,k)}}
\end{gathered}
$$

Overall, for any word $$w_i$$ with given context word $$c$$ as input,

$$
\begin{gathered}
 {\bf P(\frac{w_i}{c})} = {\bf y_i} = \frac{e^{u_i}}{\sum_{i=1}^{V} e^{u_i}} \quad where, {\bf u_i} = {\bf v_{w_i}^T.v_c}
\end{gathered}
$$

The parameters $ {\bf \theta} = {\bf \{V_w, V_c\}}$ are learned by defining the objective function as the log-likelihood and finding its gradient as

$$
\begin{gathered}
{\bf l(\theta)} = {\bf \sum_{w \in Vocabulary} log(P(\frac{w}{c}))} \\
{\bf \frac{\partial l(\theta)}{\partial V_w}} = {\bf V_c(1-P(\frac{w}{c}))}
\end{gathered}
$$

In the general CBOW model, all the one-hot vectors of context words are taken as input simultaneously, i.e,

$$
{\bf h} = {\bf W^T(x_1 + x_2 + ... + x_c)}
$$

One limitation of individual word embeddings is their inability to represent phrases [CITE], where the combination of two or more words (e.g., idioms like "hot potato" or named entities such as "Boston Globe") does not represent the combination of meanings of individual words. One solution to this problem, as explored by [CITE], is to identify such phrases based on word co-occurrence and train embeddings for them separately. More recent methods have explored directly learning n-gram embeddings from unlabeled data [CITE]. 

Another limitation comes from learning embeddings based only on a small window of surrounding words, sometimes words such as *good* and *bad* share almost the same embedding [CITE], which is problematic if used in tasks such as sentiment analysis [CITE]. At times these embeddings cluster semantically-similar words which have opposing sentiment polarities. This leads the downstream model used for the sentiment analysis task to be unable to identify this contrasting polarities leading to poor performance. [CITE] addresses this problem by proposing sentiment specific word embedding (SSWE). Authors incorporate the supervised sentiment polarity of text in their loss functions while learning the embeddings.

A general caveat for word embeddings is that they are highly dependent on the applications in which it is used. [CITE] proposed task specific embeddings which retrain the word embeddings to align them in the current task space. This is very important as training embeddings from scratch requires large amount of time and resource. [CITE] tried to address this issue by proposing *negative sampling* which is nothing but frequency-based sampling of negative terms while training the word2vec model. 

Traditional word embedding algorithms assign a distinct vector to each word. This makes them unable to account for polysemy. In a recent work, [CITE] provided an innovative way to address this deficit. The authors leveraged multilingual parallel data to learn multi-sense word embeddings. For example, the English word bank, when translated to French provides two different words: *banc* and *banque* representing financial and geographical meanings, respectively. Such multilingual distributional information helped them in accounting for polysemy.

Table [REF] provides a directory of existing frameworks that are frequently used for creating embeddings which are further incorporated into deep learning models.

<table>
  <tr>
    <td>One</td>
    <td>Two</td>
  </tr>
  <tr>
    <td colspan="2">Three</td>
  </tr>
</table>

