---
title: Convolutional Neural Networks
---

Following the popularization of word embeddings and its ability to represent words in a distributed space, the need arose for an effective feature function that extracts higher-level features from constituting words or n-grams. These abstract features would then be used for numerous NLP tasks such as sentiment analysis, summarization, machine translation, and question answering (QA). CNNs turned out to be the natural choice given their effectiveness in computer vision tasks [CITE]. 

![alt txt](img/collobertCNN.png)

The use of CNNs for sentence modeling traces back to [CITE]. This work used multi-task learning to output multiple predictions for NLP tasks such as POS tags, chunks, named-entity tags, semantic roles, semantically-similar words and a language model. A look-up table was used to transform each word into a vector of user-defined dimensions. Thus, an input sequence $$\{s_1, s_2, ... s_n\}$$ of $$n$$ words was transformed into a series of vectors $$\{ {\bf w_{s_1}}, {\bf w_{s_2}}, ... {\bf w_{s_n}} \}$$ by applying the look-up table to each of its words (Fig. [REF]). 

This can be thought of as a primitive word embedding method whose weights were learned in the training of the network. In [CITE], Collobert extended his work to propose a general CNN-based framework to solve a plethora of NLP tasks. Both these works triggered a huge popularization of CNNs amongst NLP researchers. Given that CNNs had already shown their mettle for computer vision tasks, it was easier for people to believe in their performance. 

CNNs have the ability to extract salient n-gram features from the input sentence to create an informative latent semantic representation of the sentence for downstream tasks. This application was pioneered by [CITE], which led to a huge proliferation of CNN-based networks in the succeeding literature. Below, we describe the working of a simple CNN-based sentence modeling network: 

## A. Basic CNN

### Sentence Modeling
For each sentence, let $${\bf w_{i}} \in \mathcal{R}^d$$ represent the word embedding for the $$i^{th}$$ word in the sentence, where $d$ is the dimension of the word embedding. Given that a sentence has $$n$$ words, the sentence can now be represented as an embedding matrix $${\bf W} \in \mathcal{R}^{n \times d}$$. Fig. [REF] depicts such a sentence as an input to the CNN framework.


