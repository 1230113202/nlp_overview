---
title: Performance of Different Models on Different NLP Tasks
---

We summarize the performance of a series of deep learning methods on standard datasets developed in recent years on 7 major NLP topics in Tables [REF]-[REF]. Our goal is to show the readers common datasets used in the community and state-of-the-art results with different models.

<table>
  <tr>
    <th>Paper</th>
    <th>Model</th>
    <th>WSJ-PTB (per-token accuracy %)</th>
  </tr>
  <tr>
    <td>CITE</td>
    <td>SVM with manual feature pattern</td>
    <td>97.16</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>MLP with word embeddings + CRF</td>
    <td>97.29</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>MLP with character+word embeddings</td>
    <td>97.32</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>LSTM</td>
    <td>97.29</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>Bidirectional LSTM</td>
    <td>97.40</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>LSTM-CRF</td>
    <td>97.54</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>Bidirectional LSTM-CRF</td>
    <td>97.55</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>Transition-based neural network</td>
    <td>97.45</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>DMN</td>
    <td>97.56</td>
  </tr>
</table>

*Table 2: POS tagging*

<table>
  <tr>
    <th>Parsing type</th>
    <th>Paper</th>
    <th>Model</th>
    <th><span style="font-weight:bold">WSJ</span></th>
  </tr>
  <tr>
    <td rowspan="4"><br><br><br><br>Dependency<br>Parsing</td>
    <td>CITE</td>
    <td>Fully-connected NN with features including POS</td>
    <td>91.8/89.6 (UAS/LAS)</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>Deep fully-connected NN with features including POS</td>
    <td>94.3/92.4 (UAS/LAS)</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>Stack-LSTM</td>
    <td>93.1/90.9 (UAS/LAS)</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>Beam contrastive model</td>
    <td>93.31/92.37 (UAS/LAS)</td>
  </tr>
  <tr>
    <td rowspan="4"><br><br><br><br>Constituency<br>Parsing</td>
    <td>CITE</td>
    <td>Probabilistic context-free grammars (PCFG)</td>
    <td>91.8 (F1 Score)</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>Recursive neural networks</td>
    <td>90.29 (F1 Score)</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>Feature-based transition parsing</td>
    <td>91.3 (F1 Score)</td>
  </tr>
  <tr>
    <td>CITE</td>
    <td>seq2seq learning with LSTM+Attention</td>
    <td>93.5 (F1 Score)</td>
  </tr>
</table>

*Table 3: Parsing (UAS/LAS = Unlabeled/labeled Attachment Score; WSJ = The Wall Street Journal Section of Penn Treebank)*

